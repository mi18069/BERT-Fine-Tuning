{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e44f11d",
   "metadata": {},
   "source": [
    "# Fine-Tuning Models\n",
    "Helper class that takes base model and necessary hiperparameters and returns desired fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce18b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93139b26",
   "metadata": {},
   "source": [
    "### Classification Model\n",
    "Class that takes DistilBERT model and simply adds classification head layer on top of it \n",
    "so it's ready for classification problems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ec344b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(768, 2) # output features from bert is 768 and 2 is number of labels\n",
    "        \n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        \n",
    "        last_hidden_state = self.base_model(input_ids, attention_mask=attn_mask).last_hidden_state\n",
    "        cls_embedding = last_hidden_state[:, 0, :]   # Take [CLS] token representation\n",
    "        x = self.dropout(cls_embedding)\n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120662f",
   "metadata": {},
   "source": [
    "## Full fine-tuning\n",
    "In full fine-tuning all of the model's layers are set to trainable<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b95921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_classification_model(base_model):\n",
    "    # Simply add classification head\n",
    "    model = ClassificationModel(base_model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3236c0b",
   "metadata": {},
   "source": [
    "## Parameter efficient fine-tuning\n",
    "\n",
    "Instead of training and changing all trainable parameters, we choose a subset of them or add a small set of new ones that will be adjusted, while freezing the others. The idea behind this is to have faster and computationally more effective training time.\n",
    "There are a lot of different approaches that can be used here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a639c46",
   "metadata": {},
   "source": [
    "## Classification head model\n",
    "Freezes all parameters except the ones in the last classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e405e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_head_model(base_model):\n",
    "    # Freeze all parameters\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "            \n",
    "    model = ClassificationModel(base_model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "116b845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 08:35:31.531957: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-01 08:35:32.341471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationModel(\n",
      "  (base_model): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "Trainable parameters: 1538 / 66364418\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModel.from_pretrained(\"distilbert-base-uncased\", torch_dtype=torch.float32)\n",
    "classification_model = get_classification_head_model(base_model)\n",
    "print(classification_model)\n",
    "trainable_params = sum(p.numel() for p in classification_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in classification_model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params} / {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b01906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2242403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac63e6a5",
   "metadata": {},
   "source": [
    "## Adapters\n",
    "\n",
    "Adding addapter layers after attention and feed-forward layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bda2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckAdapter(nn.Module):\n",
    "    def __init__(self, hidden_size, adapter_size, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        A bottleneck adapter module that can be inserted into a transformer.\n",
    "        \n",
    "        It projects hidden states down to a lower-dimensional space and then \n",
    "        back up again, with non-linearity and dropout in between. This helps \n",
    "        the model adapt to new tasks without updating the original transformer.\n",
    "        \n",
    "        Args:\n",
    "            hidden_size: The dimension of the model's hidden states (e.g., 768 for BERT-base)\n",
    "            adapter_size: The smaller bottleneck dimension (e.g., 64)\n",
    "            dropout_rate: Regularization to improve generalization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.down_project = nn.Linear(hidden_size, adapter_size)  # d -> b\n",
    "        self.activation = nn.GELU()  # non-linearity\n",
    "        self.up_project = nn.Linear(adapter_size, hidden_size)    # b -> d\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Initialize adapter weights â€” not learned from pretraining, so good init is important!\n",
    "        nn.init.xavier_uniform_(self.down_project.weight)\n",
    "        nn.init.zeros_(self.down_project.bias)\n",
    "        nn.init.xavier_uniform_(self.up_project.weight)\n",
    "        nn.init.zeros_(self.up_project.bias)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Store original input for residual connection\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Apply adapter: down-project -> non-linear -> up-project -> dropout\n",
    "        x = self.down_project(hidden_states)\n",
    "        x = self.activation(x)\n",
    "        x = self.up_project(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Add residual and normalize\n",
    "        output = residual + x\n",
    "        output = self.layer_norm(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc562ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterTransformerLayer(nn.Module):\n",
    "    def __init__(self, transformer_layer, adapter_size):\n",
    "        \"\"\"\n",
    "        Wraps a DistilBERT TransformerBlock with adapters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer = transformer_layer\n",
    "        self.hidden_size = transformer_layer.attention.q_lin.in_features\n",
    "\n",
    "        # Freeze the original transformer block\n",
    "        for param in self.layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Add adapters\n",
    "        self.attention_adapter = BottleneckAdapter(self.hidden_size, adapter_size)\n",
    "        self.ffn_adapter = BottleneckAdapter(self.hidden_size, adapter_size)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        # DistilBERT forward: attention -> add & norm -> ffn -> add & norm\n",
    "\n",
    "        # 1. Attention sublayer\n",
    "        sa_output = self.layer.attention(\n",
    "            hidden_states, \n",
    "            attn_mask=attention_mask, \n",
    "            head_mask=head_mask\n",
    "        )[0]\n",
    "\n",
    "        # Add + Norm (frozen)\n",
    "        sa_output = self.layer.sa_layer_norm(sa_output + hidden_states)\n",
    "\n",
    "        # Adapter after attention\n",
    "        sa_output = self.attention_adapter(sa_output)\n",
    "\n",
    "        # 2. FFN sublayer\n",
    "        ffn_output = self.layer.ffn(sa_output)\n",
    "        ffn_output = self.layer.output_layer_norm(ffn_output + sa_output)\n",
    "\n",
    "        # Adapter after FFN\n",
    "        output = self.ffn_adapter(ffn_output)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb166406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adapters_model(base_model, adapter_size=64):\n",
    "    for i in range(len(base_model.transformer.layer)):\n",
    "        original_layer = base_model.transformer.layer[i]\n",
    "        base_model.transformer.layer[i] = AdapterTransformerLayer(original_layer, adapter_size)\n",
    "    \n",
    "    classification_model = ClassificationModel(base_model)\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "977aa842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationModel(\n",
      "  (base_model): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x AdapterTransformerLayer(\n",
      "          (layer): TransformerBlock(\n",
      "            (attention): DistilBertSdpaAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (attention_adapter): BottleneckAdapter(\n",
      "            (down_project): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (activation): GELU(approximate='none')\n",
      "            (up_project): Linear(in_features=64, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (ffn_adapter): BottleneckAdapter(\n",
      "            (down_project): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (activation): GELU(approximate='none')\n",
      "            (up_project): Linear(in_features=64, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "Trainable parameters: 25045250 / 67572482\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModel.from_pretrained(\"distilbert-base-uncased\", torch_dtype=torch.float32)\n",
    "classification_model = get_adapters_model(base_model)\n",
    "print(classification_model)\n",
    "trainable_params = sum(p.numel() for p in classification_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in classification_model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params} / {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d76a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe99c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae13e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fe5129a",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "\n",
    "Apply LoRA to query and value matrices inside attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf204b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=32):\n",
    "        \"\"\"\n",
    "        LoRA implementation for linear layers.\n",
    "        \n",
    "        Args:\n",
    "            in_features: Input dimension\n",
    "            out_features: Output dimension\n",
    "            rank: Rank of the low-rank decomposition\n",
    "            alpha: Scaling factor for the LoRA contribution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # LoRA weights\n",
    "        self.lora_A = nn.Parameter(torch.zeros(in_features, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LoRA contribution: scaling * (x @ A) @ B\n",
    "        return self.scaling * (x @ self.lora_A) @ self.lora_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e16a1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, rank=8, alpha=32):\n",
    "        \"\"\"\n",
    "        Wraps a pre-trained linear layer with LoRA functionality.\n",
    "        \n",
    "        Args:\n",
    "            linear_layer: The pre-trained nn.Linear module to adapt\n",
    "            rank: Rank of the low-rank decomposition\n",
    "            alpha: Scaling factor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = linear_layer\n",
    "        \n",
    "        # Freeze original weights\n",
    "        self.linear.weight.requires_grad = False\n",
    "        if self.linear.bias is not None:\n",
    "            self.linear.bias.requires_grad = False\n",
    "            \n",
    "        # Add LoRA components\n",
    "        self.lora = LoRALayer(\n",
    "            linear_layer.in_features, \n",
    "            linear_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Combine original output with LoRA contribution\n",
    "        return self.linear(x) + self.lora(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41aacd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_model(base_model, rank=8, alpha=32, target_modules=[\"q_lin\", \"v_lin\"]):\n",
    "    \"\"\"\n",
    "    Apply LoRA to specific modules in a transformer model.\n",
    "    \n",
    "    Args:\n",
    "        model: A Hugging Face transformer model\n",
    "        rank: Rank for LoRA decomposition\n",
    "        alpha: Scaling factor\n",
    "        target_modules: List of module names to apply LoRA to\n",
    "    \"\"\"\n",
    "    # First, freeze all parameters\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Then apply LoRA to target modules\n",
    "    for name, module in base_model.named_modules():\n",
    "        if any(target_name in name for target_name in target_modules):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Get the parent module\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                child_name = name.split('.')[-1]\n",
    "                parent_module = base_model.get_submodule(parent_name)\n",
    "                \n",
    "                # Replace with LoRA version\n",
    "                lora_layer = LoRALinear(module, rank=rank, alpha=alpha)\n",
    "                setattr(parent_module, child_name, lora_layer)\n",
    "    \n",
    "    classification_model = ClassificationModel(base_model)\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "015c7a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationModel(\n",
      "  (base_model): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): LoRALinear(\n",
      "              (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): LoRALinear(\n",
      "              (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "Trainable parameters: 148994 / 66511874\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModel.from_pretrained(\"distilbert-base-uncased\", torch_dtype=torch.float32)\n",
    "classification_model = get_lora_model(base_model)\n",
    "print(classification_model)\n",
    "trainable_params = sum(p.numel() for p in classification_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in classification_model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params} / {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654260f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d47a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb464c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a1d23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca18fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
