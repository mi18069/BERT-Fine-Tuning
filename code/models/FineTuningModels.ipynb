{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e44f11d",
   "metadata": {},
   "source": [
    "# Fine-Tuning Models\n",
    "Notebook that demonstrates implementation and the core idea behind every proposed fine-tuned solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce18b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoModel\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbac684",
   "metadata": {},
   "source": [
    "### Base Model\n",
    "For the purposes of this work, we'll use BERT model which represents. It contains 110 millions parameters organized into 12 transformer layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3776c393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 18:26:02.346879: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-14 18:26:03.161312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModel.from_pretrained(\"bert-base-uncased\", dtype=torch.float32)\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66ee9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93139b26",
   "metadata": {},
   "source": [
    "### Classification Model\n",
    "Class that takes BERT model and simply adds classification head layer on top of it \n",
    "so it's ready for classification problems \n",
    "\n",
    "CLS Token\n",
    "\n",
    "The [CLS] token is a special token that is added to the beginning of the input sequence for many transformer models. Its final hidden state, after being processed by the model's layers, is used as a summary representation of the entire sequence. This is a very common technique for classification tasks of the entire input, such as sentiment analysis.\n",
    "\n",
    "Logits\n",
    "\n",
    "Logits are the raw, unnormalized scores outputted by the final linear layer of a classification model. They are a set of numbers that represent the model's confidence in each class. These values are then typically passed through a softmax function to convert them into probabilities that sum up to 1, which are much easier to interpret.\n",
    "\n",
    "Dropout\n",
    "\n",
    "Dropout is a regularization technique that helps prevent a neural network from overfitting to the training data. During training, a certain percentage of neurons in a layer (in this case, 10% on the output of the [CLS] token's embedding) are randomly \"dropped out\" or ignored. This forces the model to learn more robust and generalized features, as it can't rely on any single neuron or specific set of neurons to make a prediction.  This makes the model more effective on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec344b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(768, 2) # output features from BERT is 768 and 2 is number of labels\n",
    "\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        if self.linear.bias is not None:\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        cls_embedding = last_hidden_state[:, 0, :]   # Take [CLS] token representation\n",
    "        x = self.dropout(cls_embedding)\n",
    "        logits = self.linear(x) \n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120662f",
   "metadata": {},
   "source": [
    "## Full fine-tuning\n",
    "In full fine-tuning all of the model's layers are set to trainable<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b95921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_classification_model(base_model):\n",
    "    # Simply add classification head\n",
    "    model = ClassificationModel(base_model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2532c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 109483778 / 109483778 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "base = base_model\n",
    "classification_model = get_full_classification_model(base)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in classification_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in classification_model.parameters())\n",
    "\n",
    "trainable_params_percent = 100 * trainable_params / total_params\n",
    "print(f\"Trainable parameters: {trainable_params} / {total_params} ({trainable_params_percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3236c0b",
   "metadata": {},
   "source": [
    "## Parameter efficient fine-tuning\n",
    "\n",
    "Instead of training and changing all trainable parameters, we choose a subset of them or add a small set of new ones that will be adjusted, while freezing the others. The idea behind this is to have faster and computationally more effective training time.\n",
    "There are a lot of different approaches that can be used here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a639c46",
   "metadata": {},
   "source": [
    "## Classification head model\n",
    "Freezes all parameters except the ones in the last classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e405e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_head_model(base_model):\n",
    "    # Freeze all parameters\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "            \n",
    "    model = ClassificationModel(base_model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "116b845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1538 / 109483778 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "base = base_model\n",
    "classification_model = get_classification_head_model(base)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in classification_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in classification_model.parameters())\n",
    "\n",
    "trainable_params_percent = 100 * trainable_params / total_params\n",
    "print(f\"Trainable parameters: {trainable_params} / {total_params} ({trainable_params_percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2242403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac63e6a5",
   "metadata": {},
   "source": [
    "## Adapters\n",
    "\n",
    "Adding addapter layers after attention and feed-forward layers\n",
    "\n",
    "Key idea behind is to create \"bottleneck\" that forces model to learn a very compact, low_rank representations.\n",
    "Adapter size is the hyperparameter that defines the dimension of the down projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d7cd7",
   "metadata": {},
   "source": [
    "### AdapterModule\n",
    "\n",
    "This class represents the core of the adapter - a small bottleneck-like neural network. Proj_down layer reduces dimensionality from in_feature to given adapter_size. Through dimensionality reduction we're effectively saying that knowledge is kept on a lower-level and can be extracted without work in a large space. This is also the thing that makes adapters parameters-efficient. After performing non-linear function (ReLU or GeLU in most cases) proj_up function projects the output back to original dimension, allowing it to be added back to the main model's residual path. \n",
    "\n",
    "nn.init.zeros_\n",
    "\n",
    "By initializing the weights and biases of the proj_up layer to zero, we ensure that the adapter's initial contribution is non-existent, allowing the model to retain it's pre-trained knowledge without being fisrupted by added adapter layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bda2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterModule(nn.Module):\n",
    "    def __init__(self, in_feature, adapter_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj_down = nn.Linear(in_features=in_feature, out_features=adapter_size)\n",
    "        self.proj_up = nn.Linear(in_features=adapter_size, out_features=in_feature)\n",
    "        \n",
    "        # Setting initial values to 0 in order not to interfere with models existing knowledge\n",
    "        nn.init.zeros_(self.proj_up.weight)\n",
    "        nn.init.zeros_(self.proj_up.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj_up(F.relu(self.proj_down(x))) + x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fcff36",
   "metadata": {},
   "source": [
    "### BertLayerWithAdapters\n",
    "\n",
    "Wrapper class that takes the standard BERT transformer layer and integrate custom adapters. Two AdapterModules are initiated and placed after attention and feed-forward network sub-layer. This type of adapter implementation is called the Houlsby architecture. On the other hand, there is Pfeiffer architecture where adapters are added only after FFN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc562ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayerWithAdapters(nn.Module):\n",
    "    def __init__(self, base_layer, adapter_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_layer = deepcopy(base_layer)\n",
    "        self.adapter_size = adapter_size\n",
    "        hidden_size = self.base_layer.output.dense.out_features\n",
    "        self.attention_adapter = AdapterModule(hidden_size, adapter_size)\n",
    "        self.ffn_adapter = AdapterModule(hidden_size, adapter_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        output_attentions=False,\n",
    "        cache_position=None,\n",
    "    ):\n",
    "\n",
    "        # Call the base layer's attention sub-module\n",
    "        sa_output = self.base_layer.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            output_attentions=output_attentions,\n",
    "        )[0]\n",
    "        \n",
    "        # Apply the attention adapter and add its output to the residual path\n",
    "        adapter_attention_output = self.attention_adapter(sa_output)\n",
    "        attention_output = hidden_states + sa_output + adapter_attention_output\n",
    "        attention_output = self.base_layer.output.LayerNorm(attention_output)\n",
    "\n",
    "        # Call the base layer's feedforward sub-modules\n",
    "        intermediate_output = self.base_layer.intermediate(attention_output)\n",
    "        ffn_output = self.base_layer.output.dense(intermediate_output)\n",
    "        \n",
    "        # Apply the FFN adapter and add its output to the residual path\n",
    "        adapter_ffn_output = self.ffn_adapter(ffn_output)\n",
    "        layer_output = attention_output + ffn_output + adapter_ffn_output\n",
    "        layer_output = self.base_layer.output.LayerNorm(layer_output)\n",
    "        \n",
    "        # The return value should match the original BERT layer's output format\n",
    "        return (layer_output, ) + self.base_layer.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            output_attentions=output_attentions,\n",
    "        )[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038e4aa",
   "metadata": {},
   "source": [
    "### get_adapters_model\n",
    "\n",
    "This function takes model, and traverses through it's layers. Each model layer is replaced by a wrapper layer that contains adapter modules. \n",
    "\n",
    "At the end append classification head for execution of classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb166406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adapters_model(base_model, adapter_size=64):\n",
    "    for i, layer in enumerate(base_model.encoder.layer):\n",
    "        base_model.encoder.layer[i] = BertLayerWithAdapters(layer, adapter_size=adapter_size)\n",
    "        \n",
    "    for name, param in base_model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    for name, param in base_model.named_parameters():\n",
    "        if \"attention_adapter\" in name or \"ffn_adapter\" in name:\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    classification_model = ClassificationModel(base_model)\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "977aa842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationModel(\n",
      "  (base_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayerWithAdapters(\n",
      "          (base_layer): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (attention_adapter): AdapterModule(\n",
      "            (proj_down): Linear(in_features=768, out_features=96, bias=True)\n",
      "            (proj_up): Linear(in_features=96, out_features=768, bias=True)\n",
      "          )\n",
      "          (ffn_adapter): AdapterModule(\n",
      "            (proj_down): Linear(in_features=768, out_features=96, bias=True)\n",
      "            (proj_up): Linear(in_features=96, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "Trainable parameters: 3561218 / 113043458 (3.15%)\n"
     ]
    }
   ],
   "source": [
    "base = base_model\n",
    "classification_model = get_adapters_model(base, adapter_size=96)\n",
    "print(classification_model)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in classification_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in classification_model.parameters())\n",
    "\n",
    "trainable_params_percent = 100 * trainable_params / total_params\n",
    "print(f\"Trainable parameters: {trainable_params} / {total_params} ({trainable_params_percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d76a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fe5129a",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "\n",
    "Apply LoRA to query and value matrices inside attention layers\n",
    "\n",
    "The core idea of LoRA is to approximate the change in a weight matrix (ΔW) with a low-rank decomposition of two smaller matrices, A and B. Instead of training the entire weight matrix, only train much smaller matrices A and B. The Inner dimension of those two matrices is hyperparameter rank. Hyperparameter alpha is used as a coefficient of a degree of addition to original matrix.\n",
    "\n",
    "W_updated=W+ΔW=W+BA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfbb04b",
   "metadata": {},
   "source": [
    "### LoRALayer\n",
    "\n",
    "Creates two trainable matrices - A and B, that make up the low-rank decomposition. They are defined as nn.Parameter so they can be updated during training. A has shape (in_features, rank), while B has shape (rank, out_features).\n",
    "\n",
    "self.scaling = alpha / rank\n",
    "\n",
    "Alpha hyperparameter is used to scale the LoRA contribution. This helps to control the impact of the learned updates. Different papers propose different alpha : rank ratio.\n",
    "\n",
    "kaiming_uniform\n",
    "\n",
    "Crucial initialization steps, similar to ones for adapters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf204b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=32, alpha=64):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # LoRA weights\n",
    "        self.lora_A = nn.Parameter(torch.zeros(in_features, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LoRA contribution: scaling * (x @ A) @ B\n",
    "        return self.scaling * (x @ self.lora_A) @ self.lora_B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3f665",
   "metadata": {},
   "source": [
    "### LoRALinear\n",
    "\n",
    "Wrapper class aroung passed linear layer.\n",
    "Freezes weights and bias of the original layer and creates and adds lora layer to it which is responsible for updating weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e16a1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, rank=32, alpha=64):\n",
    "        super().__init__()\n",
    "        self.linear = linear_layer\n",
    "        \n",
    "        # Freeze original weights\n",
    "        self.linear.weight.requires_grad = False\n",
    "        if self.linear.bias is not None:\n",
    "            self.linear.bias.requires_grad = False\n",
    "            \n",
    "        # Add LoRA components\n",
    "        self.lora = LoRALayer(\n",
    "            linear_layer.in_features, \n",
    "            linear_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Combine original output with LoRA contribution\n",
    "        return self.linear(x) + self.lora(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86a6d3",
   "metadata": {},
   "source": [
    "### get_lora_model\n",
    "\n",
    "Responsible for injecting the LoRA layers into the base model\n",
    "\n",
    "It freezes all weights of the base model, so only new LoRA layers are trained.\n",
    "LoRA replaces all 'query' and 'value' linear layer in attention. In BERT implementation they are called 'query' and 'value'. \n",
    "\n",
    "At the end append classification head for execution of classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41aacd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_model(base_model, rank=32, alpha=64, target_modules=[\"query\", \"value\"]):\n",
    "    # First, freeze all parameters\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Then apply LoRA to target modules\n",
    "    for name, module in base_model.named_modules():\n",
    "        if any(target_name in name for target_name in target_modules):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Get the parent module\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                child_name = name.split('.')[-1]\n",
    "                parent_module = base_model.get_submodule(parent_name)\n",
    "                \n",
    "                # Replace with LoRA version\n",
    "                lora_layer = LoRALinear(module, rank=rank, alpha=alpha)\n",
    "                setattr(parent_module, child_name, lora_layer)\n",
    "    \n",
    "    classification_model = ClassificationModel(base_model)\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "015c7a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationModel(\n",
      "  (base_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayerWithAdapters(\n",
      "          (base_layer): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): LoRALinear(\n",
      "                  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): LoRALinear(\n",
      "                  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (attention_adapter): AdapterModule(\n",
      "            (proj_down): Linear(in_features=768, out_features=96, bias=True)\n",
      "            (proj_up): Linear(in_features=96, out_features=768, bias=True)\n",
      "          )\n",
      "          (ffn_adapter): AdapterModule(\n",
      "            (proj_down): Linear(in_features=768, out_features=96, bias=True)\n",
      "            (proj_up): Linear(in_features=96, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "Trainable parameters: 1181186 / 114223106 (1.03%)\n"
     ]
    }
   ],
   "source": [
    "base = base_model\n",
    "classification_model = get_lora_model(base, rank=32, alpha=64)\n",
    "print(classification_model)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in classification_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in classification_model.parameters())\n",
    "\n",
    "trainable_params_percent = 100 * trainable_params / total_params\n",
    "print(f\"Trainable parameters: {trainable_params} / {total_params} ({trainable_params_percent:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
