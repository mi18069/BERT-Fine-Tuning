{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e44f11d",
   "metadata": {},
   "source": [
    "# Fine-Tuning Models\n",
    "Notebook that demonstrates implementation and the core idea behind every proposed fine-tuned solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce18b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbac684",
   "metadata": {},
   "source": [
    "### Base Model\n",
    "For the purposes of this work, we'll use DistilBERT modelwhich represents smaller version of BERT model. It contains 66 millions parameters organized into 6 transformer layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3776c393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 22:36:04.972527: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-02 22:36:05.766986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): DistilBertSdpaAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModel.from_pretrained(\"distilbert-base-uncased\", torch_dtype=torch.float32)\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66ee9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93139b26",
   "metadata": {},
   "source": [
    "### Classification Model\n",
    "Class that takes DistilBERT model and simply adds classification head layer on top of it \n",
    "so it's ready for classification problems \n",
    "\n",
    "CLS Token\n",
    "\n",
    "The [CLS] token is a special token that is added to the beginning of the input sequence for many transformer models. Its final hidden state, after being processed by the model's layers, is used as a summary representation of the entire sequence. This is a very common technique for classification tasks of the entire input, such as sentiment analysis.\n",
    "\n",
    "Logits\n",
    "\n",
    "Logits are the raw, unnormalized scores outputted by the final linear layer of a classification model. They are a set of numbers that represent the model's confidence in each class. These values are then typically passed through a softmax function to convert them into probabilities that sum up to 1, which are much easier to interpret.\n",
    "\n",
    "Dropout\n",
    "\n",
    "Dropout is a regularization technique that helps prevent a neural network from overfitting to the training data. During training, a certain percentage of neurons in a layer (in this case, 10% on the output of the [CLS] token's embedding) are randomly \"dropped out\" or ignored. This forces the model to learn more robust and generalized features, as it can't rely on any single neuron or specific set of neurons to make a prediction.  This makes the model more effective on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec344b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(768, 2) # output features from bert is 768 and 2 is number of labels\n",
    "        \n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        \n",
    "        last_hidden_state = self.base_model(input_ids, attention_mask=attn_mask).last_hidden_state\n",
    "        cls_embedding = last_hidden_state[:, 0, :]   # Take [CLS] token representation\n",
    "        x = self.dropout(cls_embedding)\n",
    "        logits = self.linear(x) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120662f",
   "metadata": {},
   "source": [
    "## Full fine-tuning\n",
    "In full fine-tuning all of the model's layers are set to trainable<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b95921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_classification_model(base_model):\n",
    "    # Simply add classification head\n",
    "    model = ClassificationModel(base_model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2532c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 66364418 / 66364418 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "base = base_model\n",
    "classification_model = get_full_classification_model(base)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in classification_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in classification_model.parameters())\n",
    "\n",
    "trainable_params_percent = 100 * trainable_params / total_params\n",
    "print(f\"Trainable parameters: {trainable_params} / {total_params} ({trainable_params_percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3236c0b",
   "metadata": {},
   "source": [
    "## Parameter efficient fine-tuning\n",
    "\n",
    "Instead of training and changing all trainable parameters, we choose a subset of them or add a small set of new ones that will be adjusted, while freezing the others. The idea behind this is to have faster and computationally more effective training time.\n",
    "There are a lot of different approaches that can be used here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a639c46",
   "metadata": {},
   "source": [
    "## Classification head model\n",
    "Freezes all parameters except the ones in the last classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e405e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_head_model(base_model):\n",
    "    # Freeze all parameters\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "            \n",
    "    model = ClassificationModel(base_model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "116b845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1538 / 66364418 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "base = base_model\n",
    "classification_model = get_classification_head_model(base)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in classification_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in classification_model.parameters())\n",
    "\n",
    "trainable_params_percent = 100 * trainable_params / total_params\n",
    "print(f\"Trainable parameters: {trainable_params} / {total_params} ({trainable_params_percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2242403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac63e6a5",
   "metadata": {},
   "source": [
    "## Adapters\n",
    "\n",
    "Adding addapter layers after attention and feed-forward layers\n",
    "\n",
    "Key idea behind is to create \"bottleneck\" that forces model to learn a very compact, low_rank representations.\n",
    "Adapter size is the hyperparameter that defines the dimension of the down projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d7cd7",
   "metadata": {},
   "source": [
    "### BottleneckAdapter\n",
    "The adapter works by taking the hidden_size of the original layer (768 for DistilBERT), project it down to a smaller adapter_size dimension by down_project layer. \n",
    "Residual connection at the output allows model to learn an additive update instead of learning representation from start, which is more stable. On this way, adapter adds only a small adjustment on top of pre-trained knowledge. \n",
    "\n",
    "kaiming_uniform \n",
    "\n",
    "Since we're introducing a new layer inside the inner structure of a model, if all values are completely random there is a possibility that it can greatly downgrade the performance of the model. That's why it's necessary to initialy set some values for the adapter layer that will be able to grasp new knowledge quickly. Xavier_uniform function is an implementation of the kaiming initialization. It sets the weights of a layer based on an uniform distribution, scaled by the size of the input and output features of the layer.\n",
    "\n",
    "Bias\n",
    "\n",
    "Bias is the constant value that is added to the output of a layer's multiplication. In Linear transformation y = Wx + b, b is bias, which gives the neuron an extra degree of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bda2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckAdapter(nn.Module):\n",
    "    def __init__(self, hidden_size, adapter_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.down_project = nn.Linear(hidden_size, adapter_size)  # down projection\n",
    "        self.activation = nn.ReLU()  # non-linearity\n",
    "        self.up_project = nn.Linear(adapter_size, hidden_size)    # up projection\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Initialize adapter weights — not learned from pretraining, so good init is important!\n",
    "        nn.init.kaiming_uniform_(self.down_project.weight)\n",
    "        nn.init.zeros_(self.down_project.bias)\n",
    "        nn.init.kaiming_uniform_(self.up_project.weight)\n",
    "        nn.init.zeros_(self.up_project.bias)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Store original input for residual connection\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Apply adapter: down-project -> non-linear -> up-project -> dropout\n",
    "        x = self.down_project(hidden_states)\n",
    "        x = self.activation(x)\n",
    "        x = self.up_project(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Add residual and normalize\n",
    "        output = residual + x\n",
    "        output = self.layer_norm(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fcff36",
   "metadata": {},
   "source": [
    "### AdapterTransformerLayer\n",
    "\n",
    "Wrapper class that takes the original, pre-trained transformer layer and adds the new adapter layers.\n",
    "It freezes the weights of the original transformer layer, so they will not be updated during training.\n",
    "\n",
    "In forward function, one adapter is placed after the attention mechanism and the other after the feed-forward network (FFN), as per the original adapter paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc562ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterTransformerLayer(nn.Module):\n",
    "    def __init__(self, transformer_layer, adapter_size):\n",
    "        super().__init__()\n",
    "        self.layer = transformer_layer\n",
    "        self.hidden_size = transformer_layer.attention.q_lin.in_features\n",
    "\n",
    "        # Freeze the original transformer block\n",
    "        for param in self.layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Add adapters\n",
    "        self.attention_adapter = BottleneckAdapter(self.hidden_size, adapter_size)\n",
    "        self.ffn_adapter = BottleneckAdapter(self.hidden_size, adapter_size)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        # DistilBERT forward: attention -> add & norm -> ffn -> add & norm\n",
    "\n",
    "        # Attention sublayer\n",
    "        sa_output = self.layer.attention(\n",
    "            hidden_states, \n",
    "            attn_mask=attention_mask, \n",
    "            head_mask=head_mask\n",
    "        )[0]\n",
    "\n",
    "        # Add + Norm (frozen)\n",
    "        sa_output = self.layer.sa_layer_norm(sa_output + hidden_states)\n",
    "\n",
    "        # Adapter after attention\n",
    "        sa_output = self.attention_adapter(sa_output)\n",
    "\n",
    "        # FFN sublayer\n",
    "        ffn_output = self.layer.ffn(sa_output)\n",
    "        ffn_output = self.layer.output_layer_norm(ffn_output + sa_output)\n",
    "\n",
    "        # Adapter after FFN\n",
    "        output = self.ffn_adapter(ffn_output)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038e4aa",
   "metadata": {},
   "source": [
    "### get_adapters_model\n",
    "\n",
    "This function takes model, and traverses through it's layers. Each model layer is replaced by a wrapper layer that contains adapter modules. \n",
    "\n",
    "At the end append classification head for execution of classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb166406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adapters_model(base_model, adapter_size=64):\n",
    "    for i in range(len(base_model.transformer.layer)):\n",
    "        original_layer = base_model.transformer.layer[i]\n",
    "        base_model.transformer.layer[i] = AdapterTransformerLayer(original_layer, adapter_size)\n",
    "    \n",
    "    classification_model = ClassificationModel(base_model)\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "977aa842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationModel(\n",
      "  (base_model): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x AdapterTransformerLayer(\n",
      "          (layer): TransformerBlock(\n",
      "            (attention): DistilBertSdpaAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (attention_adapter): BottleneckAdapter(\n",
      "            (down_project): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (activation): ReLU()\n",
      "            (up_project): Linear(in_features=64, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (ffn_adapter): BottleneckAdapter(\n",
      "            (down_project): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (activation): ReLU()\n",
      "            (up_project): Linear(in_features=64, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "Trainable parameters: 1209602 / 67572482 (1.79%)\n"
     ]
    }
   ],
   "source": [
    "base = base_model\n",
    "classification_model = get_adapters_model(base)\n",
    "print(classification_model)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in classification_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in classification_model.parameters())\n",
    "\n",
    "trainable_params_percent = 100 * trainable_params / total_params\n",
    "print(f\"Trainable parameters: {trainable_params} / {total_params} ({trainable_params_percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d76a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fe5129a",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "\n",
    "Apply LoRA to query and value matrices inside attention layers\n",
    "\n",
    "The core idea of LoRA is to approximate the change in a weight matrix (ΔW) with a low-rank decomposition of two smaller matrices, A and B. Instead of training the entire weight matrix, only train much smaller matrices A and B. The Inner dimension of those two matrices is hyperparameter rank. Hyperparameter alpha is used as a coefficient of a degree of addition to original matrix.\n",
    "\n",
    "W_updated=W+ΔW=W+BA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfbb04b",
   "metadata": {},
   "source": [
    "### LoRALayer\n",
    "\n",
    "Creates two trainable matrices - A and B, that make up the low-rank decomposition. They are defined as nn.Parameter so they can be updated during training. A has shape (in_features, rank), while B has shape (rank, out_features).\n",
    "\n",
    "self.scaling = alpha / rank\n",
    "\n",
    "Alpha hyperparameter is used to scale the LoRA contribution. This helps to control the impact of the learned updates. Different papers propose different alpha : rank ratio.\n",
    "\n",
    "kaiming_uniform\n",
    "\n",
    "Crucial initialization steps, similar to ones for adapters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf204b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=32):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # LoRA weights\n",
    "        self.lora_A = nn.Parameter(torch.zeros(in_features, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LoRA contribution: scaling * (x @ A) @ B\n",
    "        return self.scaling * (x @ self.lora_A) @ self.lora_B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3f665",
   "metadata": {},
   "source": [
    "### LoRALinear\n",
    "\n",
    "Wrapper class aroung passed linear layer.\n",
    "Freezes weights and bias of the original layer and creates and adds lora layer to it which is responsible for updating weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e16a1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, rank=8, alpha=32):\n",
    "        super().__init__()\n",
    "        self.linear = linear_layer\n",
    "        \n",
    "        # Freeze original weights\n",
    "        self.linear.weight.requires_grad = False\n",
    "        if self.linear.bias is not None:\n",
    "            self.linear.bias.requires_grad = False\n",
    "            \n",
    "        # Add LoRA components\n",
    "        self.lora = LoRALayer(\n",
    "            linear_layer.in_features, \n",
    "            linear_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Combine original output with LoRA contribution\n",
    "        return self.linear(x) + self.lora(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86a6d3",
   "metadata": {},
   "source": [
    "### get_lora_model\n",
    "\n",
    "Responsible for injecting the LoRA layers into the base model\n",
    "\n",
    "It freezes all weights of the base model, so only new LoRA layers are trained.\n",
    "LoRA replaces all 'query' and 'value' linear layer in attention. In DistilBERT implementation they are called 'q_lin' and 'v_lin'. For base BERT model it's 'query' and 'value'. \n",
    "\n",
    "At the end append classification head for execution of classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41aacd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_model(base_model, rank=8, alpha=32, target_modules=[\"q_lin\", \"v_lin\"]):\n",
    "    # First, freeze all parameters\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Then apply LoRA to target modules\n",
    "    for name, module in base_model.named_modules():\n",
    "        if any(target_name in name for target_name in target_modules):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Get the parent module\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                child_name = name.split('.')[-1]\n",
    "                parent_module = base_model.get_submodule(parent_name)\n",
    "                \n",
    "                # Replace with LoRA version\n",
    "                lora_layer = LoRALinear(module, rank=rank, alpha=alpha)\n",
    "                setattr(parent_module, child_name, lora_layer)\n",
    "    \n",
    "    classification_model = ClassificationModel(base_model)\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "015c7a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationModel(\n",
      "  (base_model): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x AdapterTransformerLayer(\n",
      "          (layer): TransformerBlock(\n",
      "            (attention): DistilBertSdpaAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): LoRALinear(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): LoRALinear(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (attention_adapter): BottleneckAdapter(\n",
      "            (down_project): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (activation): ReLU()\n",
      "            (up_project): Linear(in_features=64, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (ffn_adapter): BottleneckAdapter(\n",
      "            (down_project): Linear(in_features=768, out_features=64, bias=True)\n",
      "            (activation): ReLU()\n",
      "            (up_project): Linear(in_features=64, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "Trainable parameters: 148994 / 67719938 (0.22%)\n"
     ]
    }
   ],
   "source": [
    "base = base_model\n",
    "classification_model = get_lora_model(base)\n",
    "print(classification_model)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in classification_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in classification_model.parameters())\n",
    "\n",
    "trainable_params_percent = 100 * trainable_params / total_params\n",
    "print(f\"Trainable parameters: {trainable_params} / {total_params} ({trainable_params_percent:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
