@article{AIOrigins,
	author = {Roberto Cordeschi},
	title = {AI TURNS FIFTY: REVISITING ITS ORIGINS},
	journal = {Applied Artificial Intelligence},
	volume = {21},
	number = {4-5},
	pages = {259--279},
	year = {2007},
	publisher = {Taylor \& Francis},
	doi = {10.1080/08839510701252304},
	URL = {https://doi.org/10.1080/08839510701252304},
	eprint = {https://doi.org/10.1080/08839510701252304}
}

@inbook{DataInAI,
	author = {Daochen Zha and Zaid Pervaiz Bhat and Kwei-Herng Lai and Fan Yang and Xia Hu},
	title = {Data-centric AI: Perspectives and Challenges},
	booktitle = {Proceedings of the 2023 SIAM International Conference on Data Mining (SDM)},
	chapter = {},
	pages = {945-948},
	doi = {10.1137/1.9781611977653.ch106},
	URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977653.ch106},
	eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch106}
}

@misc{LLMUsageSurvey,
	author       = {{Elon University News Bureau}},
	title        = {Survey: 52\% of U.S.\ adults now use AI large language models like ChatGPT},
	howpublished = {\url{https://www.elon.edu/u/news/2025/03/12/survey-52-of-u-s-adults-now-use-ai-large-language-models-like-chatgpt/}},
	year         = {2025},
	month        = {Mar}
}

@article{LLMCustomization,
	title={Personalization and customization of llm responses},
	author={Eapen, Joel and Adhithyan, VS},
	journal={International Journal of Research Publication and Reviews},
	volume={4},
	number={12},
	pages={2617--2627},
	year={2023}
}



@misc{GPT4TechReport,
	author = {{OpenAI}},
	title = {{GPT-4 Technical Report}},
	year = {2023},
	note = {\url{https://arxiv.org/abs/2303.08774}},
	language = {english},
	hyphenation = {english}
}

@misc{LLaMATechReport,
	author = {{Meta AI}},
	title = {{Introducing llama: A foundational, 65-billion-parameter language model}},
	year = {2023},
	note = {\url{https://ai.meta.com/blog/large-language-model-llama-meta-ai/}},
	language = {english},
	hyphenation = {english}
}

@inproceedings{CostOfLLMs,
	author = {{Abi Aryan, Aakash Kumar Nain, Andy McMahon, Lucas Augusto Meyer, Herpeet Singh Sahota}},
	title = {{The Costly Dilemma: Are Large Language Models the Pay-Day Loans of MAchine Leaning?}},
	year = {2023},
	note = {\url{https://abiaryan.com/assets/EMNLP%20Submission_Non-Anon.pdf/}},
	language = {english},
	hyphenation = {english}
}

@misc{Huggingface2023,
	author={{Hugging Face}},
	title={{Hugging Face: The AI Community Building the Future}},
	year={2023},
	note={\url{https://huggingface.co/}},
	language = {english},
	hyphenation = {english}

}

@misc{BenefitOfClassificationHeadTuning,
	title={Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution}, 
	author={Ananya Kumar and Aditi Raghunathan and Robbie Jones and Tengyu Ma and Percy Liang},
	year={2022},
	eprint={2202.10054},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2202.10054}, 
}

@misc{TuningHeadResources,
	title={Tuning Pre-trained Model via Moment Probing}, 
	author={Mingze Gao and Qilong Wang and Zhenyi Lin and Pengfei Zhu and Qinghua Hu and Jingbo Zhou},
	year={2023},
	eprint={2307.11342},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2307.11342}, 
}

@article{BertModel,
	author={{Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova}},
	title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	journal={arXiv preprint arXiv:1810.04805},
	url={https://arxiv.org/abs/1810.04805}
	year={2019},
	language = {english},
	hyphenation = {english}
}

@misc{LLMListHuggingFace,
	author       = {Hugging Face},
	title        = {GPT-2 Model Card},
	year         = {2019},
	howpublished = {\url{https://huggingface.co/transformers/v2.2.0/pretrained_models.html}},
	note         = {Accessed: 2025-08-25},
}

@misc{PalmLLM,
	author       = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and others},
	title        = {PaLM: Scaling Language Modeling with Pathways},
	year         = {2022},
	month        = apr,
	howpublished = {\url{https://research.google/blog/pathways-language-model-palm-scaling-to-540-billion-parameters-for-breakthrough-performance/}},
	note         = {Google Research Blog},
}

@misc{GPT5,
	title        = {GPT-5: Advancing the Capabilities of Large Language Models},
	author       = {OpenAI},
	year         = {2025},
	url          = {https://openai.com/research/gpt-5},
	note         = {Accessed: 2025-08-24}
}

@article{LLMParameters,
	author = {Gerstmayr, Johannes and Manzl, Peter and Pieber, Michael},
	year = {2024},
	month = {01},
	pages = {249-271},
	title = {Multibody Models Generated from Natural Language},
	volume = {62},
	journal = {Multibody System Dynamics},
	doi = {10.1007/s11044-023-09962-0}
}

@misc{LLMDataConsumption,
	title={Will we run out of data? Limits of LLM scaling based on human-generated data}, 
	author={Pablo Villalobos and Anson Ho and Jaime Sevilla and Tamay Besiroglu and Lennart Heim and Marius Hobbhahn},
	year={2024},
	eprint={2211.04325},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{BPEDataCompression,
	author    = {Philip Gage},
	title     = {A New Algorithm for Data Compression},
	booktitle = {C Users Journal},
	year      = {1994},
	volume    = {12},
	number    = {2},
	pages     = {23--38}
}

@inproceedings{BPETokenization,
	title     = {Neural Machine Translation of Rare Words with Subword Units},
	author    = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)},
	year      = {2016},
	pages     = {1715--1725},
	doi       = {10.18653/v1/P16-1162},
	url       = {https://aclanthology.org/P16-1162}
}

@inproceedings{BERTic,
	title     = {{BERTić} - The Transformer Language Model for Bosnian, Croatian, Montenegrin and Serbian},
	author    = {Dimitrov, Dimitar and Ljubešić, Nikola and Stanković, Ranka and Tadić, Marko and Mitrović, Jelena},
	booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
	year      = {2022},
	pages     = {2102--2112},
	publisher = {European Language Resources Association},
	address   = {Marseille, France},
	url       = {https://aclanthology.org/2022.lrec-1.226}
}

@misc{AttentionPaper,
	title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
	author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
	year={2016},
	eprint={1409.0473},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1409.0473}, 
}

@article{RNNAndLSTM,
	title = {Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network},
	journal = {Physica D: Nonlinear Phenomena},
	volume = {404},
	pages = {132306},
	year = {2020},
	issn = {0167-2789},
	doi = {https://doi.org/10.1016/j.physd.2019.132306},
	url = {https://www.sciencedirect.com/science/article/pii/S0167278919305974},
	author = {Alex Sherstinsky}
}

@misc{LLMScalingLaws,
	title={Scaling Laws for Neural Language Models}, 
	author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
	year={2020},
	eprint={2001.08361},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2001.08361}, 
}

@inproceedings{TransformersForImageRecognition,
	title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	booktitle={International Conference on Learning Representations},
	year={2021}
}

@inproceedings{ReLU,
	title={Rectified linear units improve restricted boltzmann machines},
	author={Nair, Vinod and Hinton, Geoffrey E},
	booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
	pages={807--814},
	year={2010}
}


@article{GeLU,
	title = {Gaussian Error Linear Units (GELUs)},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	journal = {arXiv preprint arXiv:1606.08415},
	year = {2016},
	note = {Introduces the GELU activation function: \(x\,\Phi(x)\), which offers smoother transitions and improved performance across CV, NLP, and speech tasks},
}

@article{Swish,
	title = {Searching for Activation Functions},
	author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
	journal = {arXiv preprint arXiv:1710.05941},
	year = {2017},
	note = {Introduces Swish: \(f(x) = x \cdot \sigma(x)\), a smooth self-gated activation that outperforms ReLU in deep networks},
}

@book{liu2012sentiment,
	author={{Bing Liu}},
	title={{Sentiment Analysis and Opinion Mining}},
	year={2012},
	doi={10.2200/S00416ED1V01Y201204HLT016}
	language = {english},
	hyphenation = {english}
}

@inproceedings{IMDBDataset,
	title = "Learning Word Vectors for Sentiment Analysis",
	author = "Maas, Andrew L.  and
	Daly, Raymond E.  and
	Pham, Peter T.  and
	Huang, Dan  and
	Ng, Andrew Y.  and
	Potts, Christopher",
	editor = "Lin, Dekang  and
	Matsumoto, Yuji  and
	Mihalcea, Rada",
	booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2011",
	address = "Portland, Oregon, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P11-1015/",
	pages = "142--150"
}
@article{ClassificationHeadTechnique,
	title={Parameter-Efficient Tuning Makes a Good Classification Head},
	author={Wenxuan Wang and Jifan Yu and Xiang Ren},
	journal={arXiv preprint arXiv:2210.16771},
	year={2022},
	url={https://arxiv.org/abs/2210.16771}
}

@inproceedings{AdaptersTechnique,
	title={{Parameter-Efficient Transfer Learning for {NLP}},
	author={{Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Brianna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain}},
	booktitle={{Proceedings of the 36th International Conference on Machine Learning (ICML)}},
	pages={2790--2799},
	year={2019},
	url={https://arxiv.org/abs/1902.00751}
}

@article{LoRATechnique,
	title={{LoRA}: Low-Rank Adaptation of Large Language Models},
	author={{Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen}},
	journal={{arXiv preprint arXiv:2106.09685}},
	year={2021},
	url={https://arxiv.org/abs/2106.09685}
}

@article{PEFTArticle,
	title={{A Survey on Parameter Efficient Transfer Learning for NLP}},
	author={{He, Ruibin and Anastasopoulos, Antonios and Neubig, Graham}},
	journal={{arXiv preprint arXiv:2203.06904}},
	year={2022},
	url={https://arxiv.org/abs/2203.06904}
}

@article{AttentionIsAllYouNeed,
	title={{Attention is All You Need}},
	author={{Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia}},
	journal={Advances in Neural Information Processing Systems (NeurIPS 2017)},
	year={2017},
	url={https://arxiv.org/abs/1706.03762}
}

@article{T5Raffel2020,
	title={{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
	author={{Raffel, Colin and Shinn, Adam and Cohn, Trevor and others}},
	journal={Journal of Machine Learning Research},
	volume={21},
	number={140},
	year={2020},
	pages={1--67},
	url={https://arxiv.org/abs/1910.10683}
}

@inbook{HistoryOfNLP,
	author = {Johri, Prashant and Khatri, Sunil Kumar and Al-Taani, Ahmad and Sabharwal, Munish and Suvanov, Shakhzod and Chauhan, Avneesh},
	year = {2021},
	month = {01},
	pages = {365-375},
	title = {Natural Language Processing: History, Evolution, Application, and Future Work},
	isbn = {978-981-15-9711-4},
	doi = {10.1007/978-981-15-9712-1_31}
}

@article{NLG,
	title={Natural language generation: an overview},
	author={Semaan, Paul},
	journal={J Comput Sci Res},
	volume={1},
	number={3},
	pages={50--57},
	year={2012}
}

@inproceedings{NLU,
	title = "Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data",
	author = "Bender, Emily M.  and
	Koller, Alexander",
	editor = "Jurafsky, Dan  and
	Chai, Joyce  and
	Schluter, Natalie  and
	Tetreault, Joel",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.acl-main.463/",
	doi = "10.18653/v1/2020.acl-main.463",
	pages = "5185--5198",    
}

@article{Tokenization,
	title={Critical tokenization and its properties},
	author={Guo, Jin},
	journal={Computational Linguistics},
	volume={23},
	number={4},
	pages={569--596},
	year={1997}
}

@article{Tokenizers,
	title={Tokenizer Choice For LLM Training: Negligible or Crucial?},
	author={Ali, Mehdi and Fromm, Michael and Thellmann, Klaudia and Rutmann, Richard and L{\"u}bbering, Max and Leveling, Johannes and Klug, Katrin and Ebert, Jan and Doll, Niclas and Buschhoff, Jasper Schulze and others},
	journal={arXiv preprint arXiv:2310.08754},
	year={2023}
}

@misc{Huggingface_tokenizers,
	author       = {Hugging Face},
	title        = {Tokenizers — Hugging Face NLP Course},
	year         = {2023},
	howpublished = {\url{https://huggingface.co/learn/nlp-course/en/chapter2/4}},
	note         = {Accessed: 2025-01-28}
}

@article{FFN,
	title = {Introduction to multi-layer feed-forward neural networks},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	volume = {39},
	number = {1},
	pages = {43-62},
	year = {1997},
	issn = {0169-7439},
	doi = {https://doi.org/10.1016/S0169-7439(97)00061-0},
	url = {https://www.sciencedirect.com/science/article/pii/S0169743997000610},
	author = {Daniel Svozil and Vladimír Kvasnicka and Jiri Pospichal}
}

@inproceedings{LayerNorm,
	title={On Layer Normalization in the Transformer Architecture},
	author={Xiong, Ruibin and Yang, Yi and He, Di and Zheng, Kai and Zheng, Shuo and Xing, Chao and Zhang, Hang and Lan, Yelong and Wang, Liwei and Liu, Tie-Yan and Sun, Maosong},
	booktitle={International Conference on Machine Learning},
	year={2020},
	url={https://arxiv.org/abs/2002.04745}
}

@inproceedings{ELMo,
	title = "Deep contextualized word representations",
	author = "Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke",
	booktitle = "Proceedings of NAACL-HLT",
	year = "2018",
	pages = "2227--2237"
}

@inproceedings{DistilBERT,
	title     = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
	author    = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
	booktitle = {NeurIPS EMC2 Workshop},
	year      = {2019},
	url       = {https://arxiv.org/abs/1910.01108}
}

@inproceedings{ModelDistillation,
	title={Distilling the knowledge in a neural network},
	author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	booktitle={arXiv preprint arXiv:1503.02531},
	year={2015}
}

@article{SentimentAnalysisApplication,
	author = {Feldman, Ronen},
	title = {Techniques and applications for sentiment analysis},
	year = {2013},
	issue_date = {April 2013},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {56},
	number = {4},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/2436256.2436274},
	doi = {10.1145/2436256.2436274},
	journal = {Commun. ACM},
	month = apr,
	pages = {82–89},
	numpages = {8}
}

@inproceedings{SentimentAnalysisGranularity,
	title={Sentiment analysis approaches based on granularity levels},
	author={Rachid, Benaissa Azzeddine and Azza, Harbaoui and Henda, Ben Ghezala},
	booktitle={Proceedings of the 14th International Conference on Web Information Systems and Technologies},
	volume={1},
	pages={324--331},
	year={2018}
}

@article{SentimentAnalysisChallenges,
	title = {A survey on sentiment analysis challenges},
	journal = {Journal of King Saud University - Engineering Sciences},
	volume = {30},
	number = {4},
	pages = {330-338},
	year = {2018},
	issn = {1018-3639},
	doi = {https://doi.org/10.1016/j.jksues.2016.04.002},
	url = {https://www.sciencedirect.com/science/article/pii/S1018363916300071},
	author = {Doaa Mohey El-Din Mohamed Hussein}
}

@inproceedings{SANaiveBayes,
	author={Surya, Prabha PM and Subbulakshmi, B},
	booktitle={2019 International Conference on Vision Towards Emerging Trends in Communication and Networking (ViTECoN)}, 
	title={Sentimental Analysis using Naive Bayes Classifier}, 
	year={2019},
	volume={},
	number={},
	pages={1-5},
	doi={10.1109/ViTECoN.2019.8899618}
}

@article{SASVM,
	title={Sentiment analysis of tweets using svm},
	author={Ahmad, Munir and Aftab, Shabib and Ali, Iftikhar},
	journal={Int. J. Comput. Appl},
	volume={177},
	number={5},
	pages={25--29},
	year={2017}
}

@inproceedings{SAConvolutional,
	author={Ouyang, Xi and Zhou, Pan and Li, Cheng Hua and Liu, Lijun},
	booktitle={2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing}, 
	title={Sentiment Analysis Using Convolutional Neural Network}, 
	year={2015},
	volume={},
	number={},
	pages={2359-2364},
	doi={10.1109/CIT/IUCC/DASC/PICOM.2015.349}
}

@article{SARecurrent,
	doi = {10.1088/1742-6596/1471/1/012018},
	url = {https://dx.doi.org/10.1088/1742-6596/1471/1/012018},
	year = {2020},
	month = {feb},
	publisher = {IOP Publishing},
	volume = {1471},
	number = {1},
	pages = {012018},
	author = {Kurniasari, Lilis and Setyanto, Arif},
	title = {Sentiment Analysis using Recurrent Neural Network},
	journal = {Journal of Physics: Conference Series}
}

@article{SALSTM,
	title={Text based sentiment analysis using LSTM},
	author={Murthy, GSN and Allu, Shanmukha Rao and Andhavarapu, Bhargavi and Bagadi, Mounika and Belusonti, Mounika},
	journal={Int. J. Eng. Res. Tech. Res},
	volume={9},
	number={05},
	pages={299--303},
	year={2020}
}


@inproceedings{SSTDataset,
	title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
	author = "Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher",
	booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
	year = "2013",
	pages = "1631--1642",
	url = "https://aclanthology.org/D13-1170",
}

@inproceedings{AmazonDataset,
	author={Haque, Tanjim Ul and Saber, Nudrat Nawal and Shah, Faisal Muhammad},
	booktitle={2018 IEEE International Conference on Innovative Research and Development (ICIRD)}, 
	title={Sentiment analysis on large scale Amazon product reviews}, 
	year={2018},
	volume={},
	number={},
	pages={1-6},
	doi={10.1109/ICIRD.2018.8376299}
}

@inproceedings{Sentiment140Dataset,
	author={Pal, Himanshu and Bhushan, Bharat},
	booktitle={2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT)}, 
	title={Sentiment Analysis on Twitter Dataset using Voting Classifier}, 
	year={2024},
	volume={1},
	number={},
	pages={1-6},
	keywords={Sentiment analysis;Logistic regression;Analytical models;Social networking (online);Blogs;Predictive models;Market research;Boosting;Bayes methods;Convolutional neural networks;Sentiment Analysis;TF-IDF;Ensemble methodology;Gradient Boosting},
	doi={10.1109/ICEECT61758.2024.10739316}
}
  
@inproceedings{howard-ruder-2018-universal,
	title = "Universal Language Model Fine-tuning for Text Classification",
	author = "Howard, Jeremy  and
	Ruder, Sebastian",
	editor = "Gurevych, Iryna  and
	Miyao, Yusuke",
	booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P18-1031/",
	doi = "10.18653/v1/P18-1031",
	pages = "328--339"
}

@inproceedings{peters-etal-2018-deep,
	title = "Deep Contextualized Word Representations",
	author = "Peters, Matthew E.  and
	Neumann, Mark  and
	Iyyer, Mohit  and
	Gardner, Matt  and
	Clark, Christopher  and
	Lee, Kenton  and
	Zettlemoyer, Luke",
	editor = "Walker, Marilyn  and
	Ji, Heng  and
	Stent, Amanda",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N18-1202/",
	doi = "10.18653/v1/N18-1202",
	pages = "2227--2237"
}

@misc{brown2020languagemodelsfewshotlearners,
	title={Language Models are Few-Shot Learners}, 
	author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
	year={2020},
	eprint={2005.14165},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2005.14165}, 
}

@article{BioBERT,
	author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
	title = {BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
	journal = {Bioinformatics},
	volume = {36},
	number = {4},
	pages = {1234-1240},
	year = {2019},
	month = {09},
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/btz682},
	url = {https://doi.org/10.1093/bioinformatics/btz682},
	eprint = {https://academic.oup.com/bioinformatics/article-pdf/36/4/1234/48983216/bioinformatics\_36\_4\_1234.pdf},
}

@article{radford2019language,
	title={Language models are unsupervised multitask learners},
	author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
	journal={OpenAI blog},
	volume={1},
	number={8},
	pages={9},
	year={2019}
}

@article{zhai2023catastrophicforgetting,
	title={Investigating the catastrophic forgetting in multimodal large language models},
	author={Zhai, Yuexiang and Tong, Shengbang and Li, Xiao and Cai, Mu and Qu, Qing and Lee, Yong Jae and Ma, Yi},
	journal={arXiv preprint arXiv:2309.10313},
	year={2023}
}

@misc{christophe2024med42evaluatingfinetuning,
	title={Med42 -- Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches}, 
	author={Clément Christophe and Praveen K Kanithi and Prateek Munjal and Tathagata Raha and Nasir Hayat and Ronnie Rajan and Ahmed Al-Mahrooqi and Avani Gupta and Muhammad Umar Salman and Gurpreet Gosal and Bhargav Kanakiya and Charles Chen and Natalia Vassilieva and Boulbaba Ben Amor and Marco AF Pimentel and Shadab Khan},
	year={2024},
	eprint={2404.14779},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2404.14779}, 
}

@misc{mosbach2021stabilityfinetuningbert,
	title={On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines}, 
	author={Marius Mosbach and Maksym Andriushchenko and Dietrich Klakow},
	year={2021},
	eprint={2006.04884},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2006.04884}, 
}

@misc{zaken2022bitfitspeft,
	title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models}, 
	author={Elad Ben Zaken and Shauli Ravfogel and Yoav Goldberg},
	year={2022},
	eprint={2106.10199},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2106.10199}, 
}

@misc{bapna2019adapteradaptation,
	title={Simple, Scalable Adaptation for Neural Machine Translation}, 
	author={Ankur Bapna and Naveen Arivazhagan and Orhan Firat},
	year={2019},
	eprint={1909.08478},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1909.08478}, 
}

@misc{pfeiffer2020adapterapplications,
	title={AdapterHub: A Framework for Adapting Transformers}, 
	author={Jonas Pfeiffer and Andreas Rücklé and Clifton Poth and Aishwarya Kamath and Ivan Vulić and Sebastian Ruder and Kyunghyun Cho and Iryna Gurevych},
	year={2020},
	eprint={2007.07779},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2007.07779}, 
}

@misc{pfeiffer2021adapternondestructive,
	title={AdapterFusion: Non-Destructive Task Composition for Transfer Learning}, 
	author={Jonas Pfeiffer and Aishwarya Kamath and Andreas Rücklé and Kyunghyun Cho and Iryna Gurevych},
	year={2021},
	eprint={2005.00247},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2005.00247}, 
}

@article{he2021adaptereffectiveness,
	title={On the effectiveness of adapter-based tuning for pretrained language model adaptation},
	author={He, Ruidan and Liu, Linlin and Ye, Hai and Tan, Qingyu and Ding, Bosheng and Cheng, Liying and Low, Jia-Wei and Bing, Lidong and Si, Luo},
	journal={arXiv preprint arXiv:2106.03164},
	year={2021}
}

@inproceedings{2023dettmersQLora,
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
	pages = {10088--10115},
	publisher = {Curran Associates, Inc.},
	title = {QLoRA: Efficient Finetuning of Quantized LLMs},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf},
	volume = {36},
	year = {2023}
}

@misc{zhang2023adalora,
	title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning}, 
	author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
	year={2023},
	eprint={2303.10512},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2303.10512}, 
}

@misc{liu2024alora,
	title={ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models}, 
	author={Zequan Liu and Jiawen Lyn and Wei Zhu and Xing Tian and Yvette Graham},
	year={2024},
	eprint={2403.16187},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2403.16187}, 
}

@misc{microsoft_lora,
	title = {LoRA: Low-Rank Adaptation of Large Language Models},
	author = {Microsoft},
	howpublished = {\url{https://github.com/microsoft/LoRA}},
	year = {2021},
	note = {Accessed: 2025-08-30}
}

@misc{unsloth_lora,
	author = {Unsloth Documentation},
	title = {LoRA Hyperparameters Guide},
	year = {2025},
	note = {\url{https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide}}
}

@inproceedings{regularizationtechniques,
	author = {Wu, Xuansheng and Yu, Wenhao and Zhai, Xiaoming and Liu, Ninghao},
	title = {Self-Regularization with Sparse Autoencoders for Controllable LLM-based Classification},
	year = {2025},
	isbn = {9798400714542},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3711896.3737120},
	doi = {10.1145/3711896.3737120},
	booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
	pages = {3250–3260},
	numpages = {11}
}

@INPROCEEDINGS{gridsearchandbayes,
	author={Kalele, Nupur and Kalyani, Vijay Kumar},
	booktitle={2025 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)}, 
	title={Grid Search and LLM Assisted Hybrid Approach for Hyperparameter Optimization}, 
	year={2025},
	volume={3},
	number={},
	pages={1-5},
	doi={10.1109/IATMSI64286.2025.10985615}
}

@article{biasandmitigation,
	author = {Tripathi, Manish and Agarwal, Er},
	year = {2025},
	month = {05},
	pages = {},
	title = {Bias Mitigation in NLP: Automated Detection and Correction},
	volume = {13},
	journal ={International Journal of Research in Modern Engineering \& Emerging Technology},
	doi = {10.63345/ijrmeet.org.v13.i5.130503}
}

@article{ding2023diagnosing,
	title={Diagnosing crop diseases based on domain-adaptive pre-training BERT of electronic medical records},
	author={Ding, Junqi and Li, Bo and Xu, Chang and Qiao, Yan and Zhang, Lingxian},
	journal={Applied Intelligence},
	volume={53},
	number={12},
	pages={15979--15992},
	year={2023},
	publisher={Springer}
}

@article{negativetransfer,
	author={Zhang, Wen and Deng, Lingfei and Zhang, Lei and Wu, Dongrui},
	journal={IEEE/CAA Journal of Automatica Sinica}, 
	title={A Survey on Negative Transfer}, 
	year={2023},
	volume={10},
	number={2},
	pages={305-329},
	doi={10.1109/JAS.2022.106004}
}

@misc{2025calibration,
	title={Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles}, 
	author={Yuxi Xia and Pedro Henrique Luz de Araujo and Klim Zaporojets and Benjamin Roth},
	year={2025},
	eprint={2501.03991},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2501.03991}, 
}

@misc{pytorch_nn_module,
  title        = {torch.nn — PyTorch documentation},
  year         = {2025},
  url          = {https://pytorch.org/docs/stable/nn.html#torch.nn.Module},
  note         = {Accessed: 2025-08-30}
}

@misc{pytorch,
	title	= {PyTorch documentation},
	year    = {2025},
	url     = {https://docs.pytorch.org/docs/stable/index.html},
	note    = {Accessed: 2025-08-30}
}

@article{LLMCarbonFootprint,
	title = {Preventing the Immense Increase in the Life-Cycle Energy and Carbon Footprints of LLM-Powered Intelligent Chatbots},
	journal = {Engineering},
	volume = {40},
	pages = {202-210},
	year = {2024},
	issn = {2095-8099},
	doi = {https://doi.org/10.1016/j.eng.2024.04.002},
	url = {https://www.sciencedirect.com/science/article/pii/S2095809924002315},
	author = {Peng Jiang and Christian Sonne and Wangliang Li and Fengqi You and Siming You},
	keywords = {Large language models, Intelligent chatbots, Carbon emissions, Energy and environmental footprints, Life-cycle assessment, Global cooperation}
}

@misc{2024qdylora,
      title={QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning}, 
      author={Hossein Rajabzadeh and Mojtaba Valipour and Tianshu Zhu and Marzieh Tahaei and Hyock Ju Kwon and Ali Ghodsi and Boxing Chen and Mehdi Rezagholizadeh},
      year={2024},
      eprint={2402.10462},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.10462}, 
}


